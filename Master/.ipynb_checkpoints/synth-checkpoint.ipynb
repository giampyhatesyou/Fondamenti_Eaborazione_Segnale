{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrea Giampietro, 5208458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analisi del sound processing e del funzionamento di un sintetizzatore</h1>\n",
    "<img src=\"sfondo_synth.jpg\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Obiettivi del progetto</h2>\n",
    "<ul>\n",
    "    <li>Studio della sintesi del suono</li>\n",
    "    <li>Provare a ricreare un sintetizzatore ed effetti di base per analizzarli</li>\n",
    "    <li>Confrontare il tutto con gli strumenti utilizzati sul campo</h2>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Come viene generato un suono?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Un sintetizzatore può produrre suoni fondamentali o \"onde\" di base. Le onde più comuni includono:</h4>\n",
    "    <ul>\n",
    "    <li>Onde sinusoidali: Sono onde puramente tonali e prive di armonici.\n",
    "    <li>Onde quadrate: Caratterizzate da un suono ricco di armonici pari.\n",
    "    <li>Onde a sega: Hanno una serie di armonici in crescita o decrescita.\n",
    "    <li>Onde triangolari: Simili alle onde a sega, ma con meno armonici.\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ricreiamo un semplice sintetizzatore</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44/1683502531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rate = 44100\n",
    "duration = 0.25\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con l'aiuto di scipy, nello specifico di signal, sono riuscito ad ottenere una funzione che genera forme d'onda differenti in base ai parametri di input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synth(form, note, duration):\n",
    "    x = np.linspace(0, duration, sample_rate * duration)\n",
    "    if form == 'square':\n",
    "        y = signal.square(2 * np.pi * note * x)\n",
    "    elif form == 'sine':\n",
    "        y = np.sin(2 * np.pi * note * x)\n",
    "    elif form == 'sawtooth':\n",
    "        y = signal.sawtooth(2 * np.pi * note * x)\n",
    "    elif form == 'triangle':\n",
    "        y = np.abs(signal.sawtooth(2 * np.pi * note * x * 0.5))\n",
    "    return y\n",
    "\n",
    "audio_sq = synth('square', 440, 1) #A\n",
    "IPython.display.Audio(audio_sq, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analisi delle possibili forme d'onda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((pl1, pl2), (pl3, pl4)) = plt.subplots(2, 2, figsize=(15, 6))\n",
    "\n",
    "pl1.plot(audio_sq)\n",
    "pl1.set_xlim(0, 250)\n",
    "pl1.set_xlabel('Sample')\n",
    "pl1.set_ylabel('Amplitude')\n",
    "pl1.set_title('Square Wave')\n",
    "\n",
    "#Generiamo anche un segnale sinusoidale\n",
    "audio_sin = synth('sine', 440, 1) #A\n",
    "\n",
    "pl2.plot(audio_sin, color='red')\n",
    "pl2.set_xlim(0, 250)\n",
    "pl2.set_xlabel('Sample')\n",
    "pl2.set_ylabel('Amplitude')\n",
    "pl2.set_title('Sine Wave')\n",
    "\n",
    "#Generiamo anche un segnale triangolare\n",
    "audio_tri = synth('triangle', 440, 1) #A\n",
    "pl3.plot(audio_tri, color='green')\n",
    "pl3.set_xlim(0, 250)\n",
    "pl3.set_xlabel('Sample')\n",
    "pl3.set_ylabel('Amplitude')\n",
    "pl3.set_title('Triangle Wave')\n",
    "\n",
    "#Generiamo anche un segnale a dente di sega\n",
    "audio_saw = synth('sawtooth', 440, 1) #A\n",
    "pl4.plot(audio_saw, color='orange')\n",
    "pl4.set_xlim(0, 250)\n",
    "pl4.set_xlabel('Sample')\n",
    "pl4.set_ylabel('Amplitude')\n",
    "pl4.set_title('Sawtooth Wave')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Come facciamo ad avere la nota che vogliamo?</h3>\n",
    "<h4>E' più semplice di quanto sembra, visto che quelle che noi chiamiamo note non sono altro che frequenze differenti</h4>\n",
    "\n",
    "<img src = 'note_freq.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_notes = [440, 554.37, 659.26]  # A, C#, E\n",
    "chord_duration = 2  #durata in secondi\n",
    "\n",
    "chord_audio = np.zeros(sample_rate * chord_duration)\n",
    "for note in chord_notes:\n",
    "    chord_audio += synth('square', note, chord_duration)\n",
    "\n",
    "#Aggiungiamo un sub-bass\n",
    "chord_audio += synth('sine', 110, chord_duration)\n",
    "#Aggiungiamo un alto sawtooth\n",
    "chord_audio += synth('sawtooth', 880, chord_duration)\n",
    "IPython.display.Audio(chord_audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fondamenti di sound processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prima di poter processare il suono dobbiamo capire \"dove siamo\" nello spettro delle frequenze</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "import librosa\n",
    "\n",
    "# Applichiamo una trasformata di Fourier\n",
    "chord_freq = np.fft.fftfreq(len(chord_audio), 1/sample_rate)\n",
    "chord_audio_transf = np.fft.fft(chord_audio/len(chord_audio))\n",
    "chord_audio_transf = np.fft.fftshift(chord_audio_transf)\n",
    "chord_freq = np.fft.fftshift(chord_freq)\n",
    "N = len(chord_audio)\n",
    "f  = sample_rate/N * np.arange(N)\n",
    "\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "pl1.plot(chord_freq, np.abs(chord_audio_transf))\n",
    "pl1.set_xlim(0, 20000)\n",
    "pl1.set_xlabel('Frequency (Hz)')\n",
    "pl1.set_ylabel('Amplitude')\n",
    "\n",
    "#vediamolo più da vicino\n",
    "\n",
    "#Troviamo i picchi\n",
    "peaks, _ = find_peaks(np.abs(chord_audio_transf), height=0.15)\n",
    "#Tengo solo i picchi positivi\n",
    "peaks = peaks[chord_freq[peaks] > 0]\n",
    "\n",
    "pl2.plot(chord_freq, np.abs(chord_audio_transf), color='red')\n",
    "\n",
    "pl2.set_title('Vista ravvicinata')\n",
    "pl2.set_xlim(0, 1000)\n",
    "pl2.set_xlabel('Frequency (Hz)')\n",
    "pl2.set_ylabel('Amplitude')\n",
    "pl2.set_xticks(chord_freq[peaks], labels=librosa.hz_to_note(chord_freq[peaks]))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grazie alla funzione hz_to_note() di librosa, possiamo osservare come i picchi corrispondando alle frequenze relative alle note dell'accordo.\n",
    "Notiamo inoltre come il segnale non sia in realtà composto da solo le tre note \"suonate\", ma da queste (e non solo) che vengono ripetute periodicamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applicazione di effetti di base</h3>\n",
    "<h5>Applichiamo un filtro passa banda, nello specifico terremo tutto tra i 200 e i 15k Hz</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagliamo sotto i 15000Hz e sopra i 200Hz\n",
    "ftfilt = chord_audio_transf.copy()\n",
    "indici = np.abs(chord_freq) > 15000\n",
    "indice2 = np.abs(chord_freq) < 200\n",
    "ftfilt[indici] = 0\n",
    "ftfilt[indice2] = 0\n",
    "\n",
    "\n",
    "#Stampa del grafico per i confronti\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "#Veidiamolo da vicino\n",
    "pl1.plot(chord_freq, np.abs(ftfilt))\n",
    "pl1.set_xlim(0, 20000)\n",
    "pl1.set_xlabel('f [Hz]')\n",
    "\n",
    "#Vediamo i picchi\n",
    "peaks, _ = find_peaks(np.abs(ftfilt), height=0.2)\n",
    "#Tengo solo i picchi positivi\n",
    "peaks = peaks[chord_freq[peaks] > 0]\n",
    "pl2.plot(chord_freq, np.abs(ftfilt))\n",
    "\n",
    "pl2.set_title('Vista ravvicinata')\n",
    "pl2.plot(chord_freq, np.abs(ftfilt), color='red')\n",
    "pl2.set_xlabel('f [Hz]')\n",
    "pl2.set_xlim(0, 1500)\n",
    "pl2.set_xticks(chord_freq[peaks], labels=librosa.hz_to_note(chord_freq[peaks]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' evidente l'assenza del picco a 110Hz, corrispondente alla nota di basso dell'accordo, ovvero A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analisi di un \"vero\" sintetizzatore</h2>\n",
    "\n",
    "<img src=\"junovst.png\" width=\"700\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importiamo un accordo da un sintettizatore\n",
    "sample_rate, juno = scipy.io.wavfile.read('juno_A.wav')\n",
    "juno = juno[:,0]\n",
    "juno = juno/np.max(juno)\n",
    "IPython.display.Audio(juno, rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo una trasformata di Fourier\n",
    "juno_freq = np.fft.fftfreq(len(juno), 1/sample_rate)\n",
    "juno_freq = np.fft.fftshift(juno_freq)\n",
    "juno_transf = np.fft.fft(juno/len(juno))\n",
    "juno_transf = np.fft.fftshift(juno_transf)\n",
    "\n",
    "\n",
    "#Stampa del grafico per i confronti\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "#Visualizziamo lo spettro\n",
    "pl1.plot(juno_freq, np.abs(juno_transf))\n",
    "pl1.set_xlabel('f [Hz]')\n",
    "pl2.set_title('Sintetizzatore senza filtri')\n",
    "pl1.set_xlim(0, 6000)\n",
    "\n",
    "#Vediamolo da vicino\n",
    "#Troviamo i picchi\n",
    "peaks, _ = find_peaks(juno_transf, height=0.008)\n",
    "#Tengo solo i picchi positivi\n",
    "peaks = peaks[juno_freq[peaks] > 0]\n",
    "#Teniamolo soli i picchi fino a 1500Hz\n",
    "peaks = peaks[juno_freq[peaks] < 1500]\n",
    "\n",
    "\n",
    "pl2.plot(juno_freq, np.abs(juno_transf), color = 'red')\n",
    "pl2.set_xlabel('f [Hz]')\n",
    "pl2.set_title('Vista ravvicinata')\n",
    "pl2.set_xlim(0, 1500)\n",
    "pl2.set_xticks(juno_freq[peaks], labels=librosa.hz_to_note(juno_freq[peaks]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le apparenti imprecisioni del grafico di destra sono dovute all'imprecisione del sintetizzatore utilizzato: avendo diverse note uguali ma allo stesso tempo leggermente \"stonate\", ha rilevato più picchi vicini\n",
    "\n",
    "Possiamo qiundi notare come il suono risulti meno preciso rispetto a quello da noi generato precedentemente, questo perchè i sintetizzatori cercano di generare un suono più musicale e tendono a \"sporcare\" il segnale per rendere il risultato più naturale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analisi del sound processing \"in studio\"</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora applichiamo degli effetti digitali inclusi in una digital audio workstation.<br>\n",
    "Guarderemo in dettaglio: compressore, riverbero e chorus\n",
    "\n",
    "<img src=\"compressore.png\" width=\"370\" height=\"200\">\n",
    "<img src=\"chorus.png\" width=\"250\" height=\"200\"> \n",
    "<img src=\"reverb.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compressore</h3> \n",
    "<un>\n",
    "Cosa ci aspettiamo?\n",
    "<li>Un intervento sulla dinamica del suono, avvicinando i picchi e i livelli più bassi del campione</li>\n",
    "</un>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importiamo il suono dopo aver applicato un compressore\n",
    "sample_rate, juno_comp = scipy.io.wavfile.read('juno_compresso.wav')\n",
    "juno_comp = juno_comp[:,0]\n",
    "juno_comp = juno_comp/np.max(juno_comp)\n",
    "IPython.display.Audio(juno_comp, rate=sample_rate)\n",
    "\n",
    "#Osserviamo la differenza tra i due suoni\n",
    "#Dinamica perchè il compressore agisce su di essa\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "pl1.plot(juno)\n",
    "pl1.set_xlabel('Sample')\n",
    "pl1.set_ylabel('Amplitude')\n",
    "pl1.set_ylim(-1, 1)\n",
    "pl1.set_title('Sintetizzatore senza filtri')\n",
    "\n",
    "pl2.plot(juno_comp, color='red')\n",
    "pl2.set_xlabel('Sample')\n",
    "pl2.set_ylabel('Amplitude')\n",
    "pl2.set_ylim(-1, 1)\n",
    "pl2.set_title('Sintetizzatore con compressore')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Osservazioni:</h4>\n",
    "<ul>\n",
    "<li>Il suono risulta effettivamente essere più compatto, dimunuendo la distanza tra i punti più bassi e i picchi del campione\n",
    "<li>Abbiamo comunque un picco iniziale, questo perchè il valore <i>\"Attack\"</i> permette di decidere con quanta prontezza il compressore agisce\n",
    "<li>L'accordo risultà naturale ma con una \"coda\" o sustain più accentuata\n",
    "<li>Se non avessi selezionato l'opzione <i>\"Makeup\"</i>, il suono sarebbe risultato più basso. Questa opzione permette di recuperare l'intensità persa durante la compressione\n",
    "<br>\n",
    "</ul>\n",
    "<b>Il compressore permette quindi di aumentare il dettaglio del campione, portando in primo piano anche suoni più lievi che altrimenti potrebbero venire persi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chorus</h3> \n",
    "<un>\n",
    "Cosa ci aspettiamo?\n",
    "<li>Modulazione del campione e con ritardi leggere per simulare una molteplicità di suoni simili, proprio come un coro </li>\n",
    "</un>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo effetto chorus\n",
    "sample_rate, juno_chorus = scipy.io.wavfile.read('juno_chorus.wav')\n",
    "juno_chorus = juno_chorus[:,0]\n",
    "juno_chorus = juno_chorus/np.max(juno_chorus)\n",
    "#IPython.display.Audio(juno_chorus, rate=sample_rate)\n",
    "\n",
    "#Osserviamo la differenza tra i due suoni\n",
    "#Fourier perchè il chorus agisce su distosione tonale e delay\n",
    "juno_chorus_transf = np.fft.fft(juno_chorus/len(juno_chorus))\n",
    "juno_chorus_transf = np.fft.fftshift(juno_chorus_transf)\n",
    "juno_freq = np.fft.fftfreq(len(juno_chorus), 1/sample_rate)\n",
    "juno_freq = np.fft.fftshift(juno_freq)\n",
    "\n",
    "juno_comp_transf = np.fft.fft(juno_comp/len(juno_comp))\n",
    "juno_comp_transf = np.fft.fftshift(juno_comp_transf)\n",
    "\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "pl1.plot(juno_freq, np.abs(juno_comp_transf))\n",
    "pl1.set_xlabel('f [Hz]')\n",
    "pl1.set_title('Sintetizzatore senza chorus')\n",
    "pl1.set_xlim(0, 1000)\n",
    "pl1.sharey(pl2)\n",
    "\n",
    "pl2.plot(juno_freq, np.abs(juno_chorus_transf), color='red')\n",
    "pl2.set_xlabel('f [Hz]')\n",
    "pl2.set_title('Sintetizzatore con chorus')\n",
    "pl2.set_xlim(0, 1000)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Osservazioni:</h4>\n",
    "<ul>\n",
    "<li>La trasformata di Fourier enfatizza l'evidente modulazione dal punto di vista tonale, che arricchisce il suono\n",
    "<li>I picchi principali rimangono effettivamente gli stessi, infatti l'effetto non snatura completamente il suono, ma crea la cosiddetta <i>\"texture\"</i>\n",
    "</ul>\n",
    "<b>Il chorus permette quindi di modificare il suono, aumentandone quindi la quantità effettiva di informazione contenuta nel segnale</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Riverbero</h3> \n",
    "<un>\n",
    "Cosa ci aspettiamo?\n",
    "<li>Un altro intervento di modulazione del segnale audio, genernado ritardi e feedback per simulare l'effetto di rifelsso del suono in un ambiente reale </li>\n",
    "</un>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, juno_reverb = scipy.io.wavfile.read('juno_reverb.wav')\n",
    "juno_reverb = juno_reverb[:,0]\n",
    "juno_reverb = juno_reverb/np.max(juno_reverb)\n",
    "#IPython.display.Audio(juno_reverb, rate=sample_rate)\n",
    "\n",
    "#Osserviamo la differenza tra i due suoni su spettrogramma\n",
    "#Spettrogramma perchè il riverbero agisce nel tempo\n",
    "\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "# Plot spectrogram for the original sound\n",
    "pl1.specgram(juno_chorus, Fs=sample_rate)\n",
    "pl1.set_xlabel('Time')\n",
    "pl1.set_ylim(0, 15000)\n",
    "pl1.set_ylabel('Frequency')\n",
    "pl1.set_title('Sintetizzatore senza riverbero')\n",
    "\n",
    "# Plot spectrogram for the sound with reverb\n",
    "pl2.specgram(juno_reverb, Fs=sample_rate)\n",
    "pl2.set_xlabel('Time')\n",
    "pl2.set_ylim(0, 15000)\n",
    "pl2.set_ylabel('Frequency')\n",
    "pl2.set_title('Sintetizzatore con riverbero')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "#Osserviamo la dinamica del suono con e senza riverbero\n",
    "fig, (pl1, pl2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "pl1.plot(juno_chorus)\n",
    "pl1.set_xlabel('Sample')\n",
    "pl1.set_ylabel('Amplitude')\n",
    "pl1.set_title('Sintetizzatore senza riverbero')\n",
    "\n",
    "pl2.plot(juno_reverb, color='red')\n",
    "pl2.set_xlabel('Sample')\n",
    "pl2.set_ylabel('Amplitude')\n",
    "pl2.set_title('Sintetizzatore con riverbero')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Osservazioni</h4>\n",
    "<ul>\n",
    "<li>Osserviamo come il picco del segnale si presenti dopo rispetto al segnale senza riverbero, questo perchè stiamo osservando quasi esclusivamente il segnale di riflesso generato dall'effetto (l'impostazione <i>Dry/Wet</i> è impostata al 91%)\n",
    "<li>Lo spettrogramma evidenza una carenza di segnale oltre i 50kHz, questo perchè l'effetto presenta un filtro che taglia il suono oltre questa soglia, per evitare di dare troppo spazio al segnalr\n",
    "<li>Dallo spettrogramma si può inoltre notare come il picco iniziale venga \"sgranato\". Questo perchè il picco durerà più a lungo e sarà quindi meno intenso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusioni</h1>\n",
    "L'idea iniziale di creare un sintetizzatore completo fai da te era avrebbe significato anche implementare i filtri che abbiamo visto ma richiederebbero un progetto ciascuno. Ho ritenuto altrettanto interessante dare un'infarinatura sul processo dietro alla sintesi dei suoni, dalla loro nascita ad alcuni effetti di base ai quali vengono sottoposti nel processo ci sound design.\n",
    "Con questo progetto spero di aver mostrato all'ascoltatore il processo, che eseguo quasi quotidianamente da anni, che segue il segnale audio prima di arrivare alle sue cuffie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
